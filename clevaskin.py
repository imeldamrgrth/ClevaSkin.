# -*- coding: utf-8 -*-
"""ClevaSkin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Axr9M34GfIFlpJr4enrn0s7hP9qQglXN
"""

from google.colab import drive
import zipfile
import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import shutil
import random
import tensorflow as tf
import cv2
import matplotlib.cm as cm
import json
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
from matplotlib import cm
from tensorflow.keras.models import load_model
from PIL import Image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from sklearn.preprocessing import label_binarize
from google.colab import files

drive.mount('/content/drive')

zip_path = '/content/drive/MyDrive/ClevaSkin/ClevaDS.zip'
extract_path = '/content/ClevaDS'

# Ekstrak ZIP
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

os.listdir(extract_path)

# Path ke folder utama
base_path = '/content/ClevaDS/ClevaDS'

# List untuk menyimpan hasil
data = []

# Loop tiap folder (kelas)
for folder_name in sorted(os.listdir(base_path)):
    folder_path = os.path.join(base_path, folder_name)
    if os.path.isdir(folder_path):
        count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
        data.append({'Kelas': folder_name, 'Jumlah Gambar': count})

# Ubah jadi DataFrame
df = pd.DataFrame(data)
print(df)

# Path ke folder utama
base_path = '/content/ClevaDS/ClevaDS'

# List penampung data
file_name = []
labels = []
full_path = []

# Looping untuk membaca semua gambar dan label
for path, subdirs, files in os.walk(base_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(os.path.basename(path))
        file_name.append(name)

# Buat DataFrame
distribution_train = pd.DataFrame({
    "path": full_path,
    "file_name": file_name,
    "labels": labels
})

# Plot distribusi gambar per kelas
plt.figure(figsize=(10, 6))
sns.set_style("darkgrid")
plot_data = sns.countplot(data=distribution_train, y='labels', order=distribution_train['labels'].value_counts().index)
plt.title("Distribusi Gambar per Kelas")
plt.xlabel("Jumlah Gambar")
plt.ylabel("Kelas")
plt.tight_layout()
plt.show()

# Path ke dataset
dataset_path = '/content/ClevaDS/ClevaDS'

# Membuat kamus gambar untuk setiap kelas
disease_images = {}

# Mengisi kamus dengan nama file dari setiap kelas
for class_folder in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_folder)
    if os.path.isdir(class_path):
        disease_images[class_folder] = os.listdir(class_path)

# Menampilkan 5 gambar acak dari setiap kelas
fig, axs = plt.subplots(len(disease_images), 5, figsize=(15, len(disease_images) * 2))

for i, (class_name, image_list) in enumerate(disease_images.items()):
    selected_images = np.random.choice(image_list, 5, replace=False)

    for j, image_name in enumerate(selected_images):
        img_path = os.path.join(dataset_path, class_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set_title(class_name, fontsize=8)
        axs[i, j].axis('off')

plt.tight_layout()
plt.show()

# Path sumber dan tujuan
source_path = base_path  # Folder asal semua kelas
target_path = '/content/balanced_dataset'
target_count = 1500  # Target per kelas

# Proses setiap kelas
for class_name in os.listdir(source_path):
    src_folder = os.path.join(source_path, class_name)
    dst_folder = os.path.join(target_path, class_name)
    os.makedirs(dst_folder, exist_ok=True)

    img_files = [f for f in os.listdir(src_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    current_count = len(img_files)

    if current_count < target_count:
        print(f"❌ {class_name}: hanya {current_count} gambar, tidak cukup untuk undersampling.")
        continue

    print(f"Undersampling {class_name}: {current_count} → {target_count}")
    selected_files = random.sample(img_files, target_count)
    for img_name in selected_files:
        shutil.copy(os.path.join(src_folder, img_name), dst_folder)

balanced_path = '/content/balanced_dataset'

data = []

for class_name in sorted(os.listdir(balanced_path)):
    class_folder = os.path.join(balanced_path, class_name)
    if os.path.isdir(class_folder):
        count = len([
            f for f in os.listdir(class_folder)
            if f.lower().endswith(('.jpg', '.jpeg', '.png'))
        ])
        data.append({"Kelas": class_name, "Jumlah Gambar": count})

df_check = pd.DataFrame(data)
print(df_check)

# Urutan kelas
df_check_sorted = df_check.sort_values(by="Jumlah Gambar", ascending=False)

# Plot
plt.figure(figsize=(12, 8))
sns.set_style("whitegrid")
plot = sns.barplot(
    data=df_check_sorted,
    x="Jumlah Gambar",
    y="Kelas",
    palette="viridis"
)

# Judul dan label
plt.title("Distribusi Jumlah Gambar per Kelas (Balanced Dataset)", fontsize=16)
plt.xlabel("Jumlah Gambar", fontsize=12)
plt.ylabel("Kelas", fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.tight_layout()
plt.show()

source_dir = '/content/balanced_dataset'
output_dir = '/content/split_dataset'
split_ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15}

# Pastikan folder tujuan dibuat
for split in split_ratios.keys():
    for class_name in os.listdir(source_dir):
        os.makedirs(os.path.join(output_dir, split, class_name), exist_ok=True)

# Lakukan pemisahan untuk setiap kelas
for class_name in os.listdir(source_dir):
    img_dir = os.path.join(source_dir, class_name)
    images = os.listdir(img_dir)

    # Split: train → val → test
    train_files, testval_files = train_test_split(images, test_size=(1 - split_ratios['train']), random_state=42)
    val_files, test_files = train_test_split(testval_files, test_size=(split_ratios['test'] / (split_ratios['val'] + split_ratios['test'])), random_state=42)

    for fname in train_files:
        shutil.copy(os.path.join(img_dir, fname), os.path.join(output_dir, 'train', class_name))
    for fname in val_files:
        shutil.copy(os.path.join(img_dir, fname), os.path.join(output_dir, 'val', class_name))
    for fname in test_files:
        shutil.copy(os.path.join(img_dir, fname), os.path.join(output_dir, 'test', class_name))

print("Dataset berhasil di-split ke train/val/test.")

split_counts = []

for split in ['train', 'val', 'test']:
    split_path = os.path.join(output_dir, split)
    for class_name in sorted(os.listdir(split_path)):
        class_path = os.path.join(split_path, class_name)
        count = len(os.listdir(class_path))
        split_counts.append({
            'Split': split,
            'Kelas': class_name,
            'Jumlah Gambar': count
        })

df_split = pd.DataFrame(split_counts)
print(df_split.pivot(index='Kelas', columns='Split', values='Jumlah Gambar'))

img_size = (224, 224)
batch_size = 32

train_gen = ImageDataGenerator(rescale=1./255)
val_gen = ImageDataGenerator(rescale=1./255)
test_gen = ImageDataGenerator(rescale=1./255)

train_data = train_gen.flow_from_directory(
    os.path.join(output_dir, 'train'),
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical'
)

val_data = val_gen.flow_from_directory(
    os.path.join(output_dir, 'val'),
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical'
)

test_data = test_gen.flow_from_directory(
    os.path.join(output_dir, 'test'),
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False  # untuk evaluasi & confusion matrix
)

class_labels = [label for label, _ in sorted(train_data.class_indices.items(), key=lambda x: x[1])]
with open("class_labels.json", "w") as f:
    json.dump(class_labels, f)

print("class_labels.json berhasil disimpan.")
print("Urutan label:", class_labels)

# Ambil jumlah kelas dari data generator
num_classes = train_data.num_classes  # otomatis dari folder

# Bangun model
base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # freeze

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
outputs = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=outputs)

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Tampilkan arsitektur model
model.summary()

# Model dengan base MobileNetV2
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Bekukan layer pre-trained

# Head klasifikasi
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
predictions = Dense(train_data.num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Compile model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Latih model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=20
)

# Evaluasi model
test_loss, test_acc = model.evaluate(test_data)
print(f"Akurasi data uji: {test_acc:.4f}")

# Simpan model
model.save('/content/skin_disease_model.h5')

# UNFREEZE base model
base_model.trainable = True

# Compile ulang dengan learning rate lebih kecil
model.compile(optimizer=Adam(learning_rate=1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Callback
earlystop = EarlyStopping(patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(patience=2, factor=0.2, verbose=1)
checkpoint_finetune = ModelCheckpoint(
    'best_model_finetuned.h5', monitor='val_accuracy', mode='max',
    save_best_only=True, verbose=1
)

# Fine-tune
fine_tune_history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=20,
    callbacks=[earlystop, reduce_lr, checkpoint_finetune]
)

# Evaluasi akhir
final_loss, final_acc = model.evaluate(test_data)
print(f"Akurasi data uji setelah fine-tuning: {final_acc:.4f}")

# Simpan model final
model.save('/conte nt/skin_disease_model_finetuned.h5')

# Test generator
test_datagen = ImageDataGenerator(rescale=1./255)

test_gen = test_datagen.flow_from_directory(
    '/content/balanced_dataset',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# Prediksi
y_pred_probs = model.predict(test_gen)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = test_gen.classes
class_labels = list(test_gen.class_indices.keys())

# Classification report
print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_labels))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
fig, ax = plt.subplots(figsize=(10, 10))
disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)
plt.title("Confusion Matrix - Model Akhir")
plt.show()

# ROC CURVE per kelas
# Binarize label untuk ROC
y_true_bin = label_binarize(y_true, classes=list(range(len(class_labels))))
fpr, tpr, roc_auc = {}, {}, {}

for i in range(len(class_labels)):
    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC per kelas
plt.figure(figsize=(12, 8))
for i in range(len(class_labels)):
    plt.plot(fpr[i], tpr[i], label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve per Class")
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Perbandingan Train vs Validation Loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='x')
plt.title('Perbandingan Train vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Perbandingan Train vs Validation Accuracy
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='x')
plt.title('Perbandingan Train vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

uploaded = files.upload()
image_path = next(iter(uploaded))

# Load image
img = load_img(image_path, target_size=(224, 224))
img_np = img_to_array(img) / 255.0
x = np.expand_dims(img_np, axis=0)


# Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name='Conv_1', pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Prediksi Awal
pred_initial = model.predict(x)[0]
sorted_initial = sorted(zip(class_labels, pred_initial), key=lambda x: x[1], reverse=True)
initial_label, initial_conf = sorted_initial[0]
print(f"\nPrediksi Awal: {initial_label} ({initial_conf:.2%})")

# Grad-CAM
heatmap = make_gradcam_heatmap(x, model)
heatmap_resized = cv2.resize(heatmap, (224, 224))
heatmap_mask = (heatmap_resized > 0.4).astype("uint8")
prop_panas = np.sum(heatmap_mask) / (224 * 224)

# Adaptive Crop
if prop_panas < 0.3 and initial_conf < 0.60:
    contours, _ = cv2.findContours(heatmap_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        x_min, y_min, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
        cropped_img = img_np[y_min:y_min+h, x_min:x_min+w]
        cropped_img_resized = cv2.resize(cropped_img, (224, 224))
        cropped_input = np.expand_dims(cropped_img_resized, axis=0)
        crop_note = "Area kecil & confidence rendah → dilakukan crop."
    else:
        cropped_img_resized = img_np
        cropped_input = x
        crop_note = "Tidak ada kontur signifikan → gambar asli dipakai."
else:
    cropped_img_resized = img_np
    cropped_input = x
    crop_note = "Gambar cukup jelas → tidak perlu crop."

# Prediksi Akhir
pred = model.predict(cropped_input)[0]
sorted_pred = sorted(zip(class_labels, pred), key=lambda x: x[1], reverse=True)
pred_label, confidence = sorted_pred[0]

# Interpretasi
normal_conf = dict(sorted_pred).get("Normal Skin", 0)
nonskin_conf = dict(sorted_pred).get("Non-Skin", 0)
unknown_conf = dict(sorted_pred).get("Unknown", 0)

hybrid_alert = ""
if confidence < 0.60:
    pred_label = "Tidak dikenali"
    hybrid_alert = "Model tidak yakin. Gambar bisa blur, noise, atau belum dikenali."
elif pred_label == "Non-Skin":
    hybrid_alert = "Gambar terdeteksi sebagai Non-Skin (bukan kulit manusia)."
elif pred_label == "Unknown":
    hybrid_alert = "Gambar terdeteksi sebagai penyakit kulit lain (di luar 13 kelas utama)."
elif pred_label == "Normal Skin" and dict(sorted_pred).get("Acne", 0) >= 0.05:
    hybrid_alert = f"Kulit tampak normal, tapi ada indikasi ringan dari acne ({dict(sorted_pred)['Acne']:.2%})"

# Cetak Prediksi
print("\nHasil Prediksi Akhir:")
for label, score in sorted_pred:
    print(f"{label:25}: {score*100:.2f}%")

# Visualisasi
plt.figure(figsize=(16, 6))

plt.subplot(1, 3, 1)
plt.imshow(cropped_img_resized)
plt.title(f"Prediksi: {pred_label}\n({confidence:.2%})", fontsize=10)
plt.axis('off')

plt.subplot(1, 3, 2)
heatmap_color = cm.jet(heatmap_resized)[..., :3]
superimposed_img = np.clip(heatmap_color * 0.4 + img_np, 0, 1)
plt.imshow(superimposed_img)
plt.title("Grad-CAM Heatmap", fontsize=10)
plt.axis('off')

plt.suptitle(hybrid_alert if hybrid_alert else crop_note, fontsize=12, y=1.05, color='red')
plt.tight_layout()
plt.show()

uploaded = files.upload()
image_path = next(iter(uploaded))

# Load image
img = load_img(image_path, target_size=(224, 224))
img_np = img_to_array(img) / 255.0
x = np.expand_dims(img_np, axis=0)


# Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name='Conv_1', pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Prediksi Awal
pred_initial = model.predict(x)[0]
sorted_initial = sorted(zip(class_labels, pred_initial), key=lambda x: x[1], reverse=True)
initial_label, initial_conf = sorted_initial[0]
print(f"\nPrediksi Awal: {initial_label} ({initial_conf:.2%})")

# Grad-CAM
heatmap = make_gradcam_heatmap(x, model)
heatmap_resized = cv2.resize(heatmap, (224, 224))
heatmap_mask = (heatmap_resized > 0.4).astype("uint8")
prop_panas = np.sum(heatmap_mask) / (224 * 224)

# Adaptive Crop
if prop_panas < 0.3 and initial_conf < 0.60:
    contours, _ = cv2.findContours(heatmap_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        x_min, y_min, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
        cropped_img = img_np[y_min:y_min+h, x_min:x_min+w]
        cropped_img_resized = cv2.resize(cropped_img, (224, 224))
        cropped_input = np.expand_dims(cropped_img_resized, axis=0)
        crop_note = "Area kecil & confidence rendah → dilakukan crop."
    else:
        cropped_img_resized = img_np
        cropped_input = x
        crop_note = "Tidak ada kontur signifikan → gambar asli dipakai."
else:
    cropped_img_resized = img_np
    cropped_input = x
    crop_note = "Gambar cukup jelas → tidak perlu crop."

# Prediksi Akhir
pred = model.predict(cropped_input)[0]
sorted_pred = sorted(zip(class_labels, pred), key=lambda x: x[1], reverse=True)
pred_label, confidence = sorted_pred[0]

# Interpretasi
normal_conf = dict(sorted_pred).get("Normal Skin", 0)
nonskin_conf = dict(sorted_pred).get("Non-Skin", 0)
unknown_conf = dict(sorted_pred).get("Unknown", 0)

hybrid_alert = ""
if confidence < 0.60:
    pred_label = "Tidak dikenali"
    hybrid_alert = "Model tidak yakin. Gambar bisa blur, noise, atau belum dikenali."
elif pred_label == "Non-Skin":
    hybrid_alert = "Gambar terdeteksi sebagai Non-Skin (bukan kulit manusia)."
elif pred_label == "Unknown":
    hybrid_alert = "Gambar terdeteksi sebagai penyakit kulit lain (di luar 13 kelas utama)."
elif pred_label == "Normal Skin" and dict(sorted_pred).get("Acne", 0) >= 0.05:
    hybrid_alert = f"Kulit tampak normal, tapi ada indikasi ringan dari acne ({dict(sorted_pred)['Acne']:.2%})"

# Cetak Prediksi
print("\nHasil Prediksi Akhir:")
for label, score in sorted_pred:
    print(f"{label:25}: {score*100:.2f}%")

# Visualisasi
plt.figure(figsize=(16, 6))

plt.subplot(1, 3, 1)
plt.imshow(cropped_img_resized)
plt.title(f"Prediksi: {pred_label}\n({confidence:.2%})", fontsize=10)
plt.axis('off')

plt.subplot(1, 3, 2)
heatmap_color = cm.jet(heatmap_resized)[..., :3]
superimposed_img = np.clip(heatmap_color * 0.4 + img_np, 0, 1)
plt.imshow(superimposed_img)
plt.title("Grad-CAM Heatmap", fontsize=10)
plt.axis('off')

plt.suptitle(hybrid_alert if hybrid_alert else crop_note, fontsize=12, y=1.05, color='red')
plt.tight_layout()
plt.show()

uploaded = files.upload()
image_path = next(iter(uploaded))

# Load image
img = load_img(image_path, target_size=(224, 224))
img_np = img_to_array(img) / 255.0
x = np.expand_dims(img_np, axis=0)


# Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name='Conv_1', pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Prediksi Awal
pred_initial = model.predict(x)[0]
sorted_initial = sorted(zip(class_labels, pred_initial), key=lambda x: x[1], reverse=True)
initial_label, initial_conf = sorted_initial[0]
print(f"\nPrediksi Awal: {initial_label} ({initial_conf:.2%})")

# Grad-CAM
heatmap = make_gradcam_heatmap(x, model)
heatmap_resized = cv2.resize(heatmap, (224, 224))
heatmap_mask = (heatmap_resized > 0.4).astype("uint8")
prop_panas = np.sum(heatmap_mask) / (224 * 224)

# Adaptive Crop
if prop_panas < 0.3 and initial_conf < 0.60:
    contours, _ = cv2.findContours(heatmap_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        x_min, y_min, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
        cropped_img = img_np[y_min:y_min+h, x_min:x_min+w]
        cropped_img_resized = cv2.resize(cropped_img, (224, 224))
        cropped_input = np.expand_dims(cropped_img_resized, axis=0)
        crop_note = "Area kecil & confidence rendah → dilakukan crop."
    else:
        cropped_img_resized = img_np
        cropped_input = x
        crop_note = "Tidak ada kontur signifikan → gambar asli dipakai."
else:
    cropped_img_resized = img_np
    cropped_input = x
    crop_note = "Gambar cukup jelas → tidak perlu crop."

# Prediksi Akhir
pred = model.predict(cropped_input)[0]
sorted_pred = sorted(zip(class_labels, pred), key=lambda x: x[1], reverse=True)
pred_label, confidence = sorted_pred[0]

# Interpretasi
normal_conf = dict(sorted_pred).get("Normal Skin", 0)
nonskin_conf = dict(sorted_pred).get("Non-Skin", 0)
unknown_conf = dict(sorted_pred).get("Unknown", 0)

hybrid_alert = ""
if confidence < 0.60:
    pred_label = "Tidak dikenali"
    hybrid_alert = "Model tidak yakin. Gambar bisa blur, noise, atau belum dikenali."
elif pred_label == "Non-Skin":
    hybrid_alert = "Gambar terdeteksi sebagai Non-Skin (bukan kulit manusia)."
elif pred_label == "Unknown":
    hybrid_alert = "Gambar terdeteksi sebagai penyakit kulit lain (di luar 13 kelas utama)."
elif pred_label == "Normal Skin" and dict(sorted_pred).get("Acne", 0) >= 0.05:
    hybrid_alert = f"Kulit tampak normal, tapi ada indikasi ringan dari acne ({dict(sorted_pred)['Acne']:.2%})"

# Cetak Prediksi
print("\nHasil Prediksi Akhir:")
for label, score in sorted_pred:
    print(f"{label:25}: {score*100:.2f}%")

# Visualisasi
plt.figure(figsize=(16, 6))

plt.subplot(1, 3, 1)
plt.imshow(cropped_img_resized)
plt.title(f"Prediksi: {pred_label}\n({confidence:.2%})", fontsize=10)
plt.axis('off')

plt.subplot(1, 3, 2)
heatmap_color = cm.jet(heatmap_resized)[..., :3]
superimposed_img = np.clip(heatmap_color * 0.4 + img_np, 0, 1)
plt.imshow(superimposed_img)
plt.title("Grad-CAM Heatmap", fontsize=10)
plt.axis('off')

plt.suptitle(hybrid_alert if hybrid_alert else crop_note, fontsize=12, y=1.05, color='red')
plt.tight_layout()
plt.show()

uploaded = files.upload()
image_path = next(iter(uploaded))

# Load image
img = load_img(image_path, target_size=(224, 224))
img_np = img_to_array(img) / 255.0
x = np.expand_dims(img_np, axis=0)


# Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name='Conv_1', pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Prediksi Awal
pred_initial = model.predict(x)[0]
sorted_initial = sorted(zip(class_labels, pred_initial), key=lambda x: x[1], reverse=True)
initial_label, initial_conf = sorted_initial[0]
print(f"\nPrediksi Awal: {initial_label} ({initial_conf:.2%})")

# Grad-CAM
heatmap = make_gradcam_heatmap(x, model)
heatmap_resized = cv2.resize(heatmap, (224, 224))
heatmap_mask = (heatmap_resized > 0.4).astype("uint8")
prop_panas = np.sum(heatmap_mask) / (224 * 224)

# Adaptive Crop
if prop_panas < 0.3 and initial_conf < 0.60:
    contours, _ = cv2.findContours(heatmap_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        x_min, y_min, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
        cropped_img = img_np[y_min:y_min+h, x_min:x_min+w]
        cropped_img_resized = cv2.resize(cropped_img, (224, 224))
        cropped_input = np.expand_dims(cropped_img_resized, axis=0)
        crop_note = "Area kecil & confidence rendah → dilakukan crop."
    else:
        cropped_img_resized = img_np
        cropped_input = x
        crop_note = "Tidak ada kontur signifikan → gambar asli dipakai."
else:
    cropped_img_resized = img_np
    cropped_input = x
    crop_note = "Gambar cukup jelas → tidak perlu crop."

# Prediksi Akhir
pred = model.predict(cropped_input)[0]
sorted_pred = sorted(zip(class_labels, pred), key=lambda x: x[1], reverse=True)
pred_label, confidence = sorted_pred[0]

# Interpretasi
normal_conf = dict(sorted_pred).get("Normal Skin", 0)
nonskin_conf = dict(sorted_pred).get("Non-Skin", 0)
unknown_conf = dict(sorted_pred).get("Unknown", 0)

hybrid_alert = ""
if confidence < 0.60:
    pred_label = "Tidak dikenali"
    hybrid_alert = "Model tidak yakin. Gambar bisa blur, noise, atau belum dikenali."
elif pred_label == "Non-Skin":
    hybrid_alert = "Gambar terdeteksi sebagai Non-Skin (bukan kulit manusia)."
elif pred_label == "Unknown":
    hybrid_alert = "Gambar terdeteksi sebagai penyakit kulit lain (di luar 13 kelas utama)."
elif pred_label == "Normal Skin" and dict(sorted_pred).get("Acne", 0) >= 0.05:
    hybrid_alert = f"Kulit tampak normal, tapi ada indikasi ringan dari acne ({dict(sorted_pred)['Acne']:.2%})"

# Cetak Prediksi
print("\nHasil Prediksi Akhir:")
for label, score in sorted_pred:
    print(f"{label:25}: {score*100:.2f}%")

# Visualisasi
plt.figure(figsize=(16, 6))

plt.subplot(1, 3, 1)
plt.imshow(cropped_img_resized)
plt.title(f"Prediksi: {pred_label}\n({confidence:.2%})", fontsize=10)
plt.axis('off')

plt.subplot(1, 3, 2)
heatmap_color = cm.jet(heatmap_resized)[..., :3]
superimposed_img = np.clip(heatmap_color * 0.4 + img_np, 0, 1)
plt.imshow(superimposed_img)
plt.title("Grad-CAM Heatmap", fontsize=10)
plt.axis('off')

plt.suptitle(hybrid_alert if hybrid_alert else crop_note, fontsize=12, y=1.05, color='red')
plt.tight_layout()
plt.show()